{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "\n",
    "def getVector(w):\n",
    "    global model\n",
    "    if w in model:\n",
    "        return model[w]\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "\n",
    "\n",
    "def cos_sim(w1,w2):\n",
    "    return np.dot(getVector(w1),getVector(w2)) / ( np.linalg.norm(getVector(w1)) * np.linalg.norm(getVector(w2)))\n",
    "\n",
    "\n",
    "def extract_from_corpus(corpus, capture_words):\n",
    "    # create bag of words and sum up word occurrences to find most frequent words in the given corpus\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    # only keep words that have a count >= 60, and have cosine similarity >= 0.28 from words relevant to the context of the subreddit\n",
    "    # (these words are themselves common in the subreddit, and were chosen based on observations made manually by sifting through the words common in the corpus)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items() if ((sum_words[0,idx] >= 60) and (cos_sim(word,capture_words[0]) >= 0.28 or cos_sim(word,capture_words[1]) >= 0.28))]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    relevant_common_words = [w[0] for w in words_freq]\n",
    "    \n",
    "    return relevant_common_words\n",
    "\n",
    "\n",
    "def classifySubreddit_train(trainFile):\n",
    "    \n",
    "    # list to store the data from the training file\n",
    "    data = []\n",
    "    \n",
    "    # collect data from json file\n",
    "    with open(trainFile, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    # corpi to store the corpus of each subreddit\n",
    "    corpus_nerf = []\n",
    "    corpus_uke = []\n",
    "    corpus_navy = []\n",
    "    corpus_mario = []\n",
    "    \n",
    "    # comments stores the processed text of all the comments\n",
    "    comments = []\n",
    "    # comment_vecs stores vectors of all the comments\n",
    "    comment_vecs = []\n",
    "    # subreddits stores the subreddit of each comment\n",
    "    subreddits = []\n",
    "    \n",
    "    # used to join text later\n",
    "    space = \" \"\n",
    "    \n",
    "    global stops\n",
    "    global zero_arr\n",
    "    \n",
    "    for d in data:\n",
    "        # remove punctuation from each comment, make it lowercase\n",
    "        comment = d['body']\n",
    "        comment = comment.lower()\n",
    "        comment = re.sub(r'[^a-zA-Z0-9 ]', ' ', comment)\n",
    "        comment = re.sub(r' +',' ', comment)\n",
    "        \n",
    "        # split comment into tokens/words and keep only the tokens that are not stopwords and have a vector in the word2vec model\n",
    "        tokens = comment.split()\n",
    "        tokens = [w for w in tokens if (w not in stops) and not (np.array_equal(w,zero_arr))]\n",
    "        \n",
    "        # join the tokens back to form the processed text of the comment\n",
    "        text = \"\"\n",
    "        text = space.join(tokens)\n",
    "        \n",
    "        # append the processed text to its respective corpus, and append the subreddit name to the list of subreddits\n",
    "        if d['subreddit'] == 'Nerf':\n",
    "            corpus_nerf.append(text)\n",
    "            subreddits.append(0)\n",
    "        elif d['subreddit'] == 'ukulele':\n",
    "            corpus_uke.append(text)\n",
    "            subreddits.append(1)\n",
    "        elif d['subreddit'] == 'newToTheNavy':\n",
    "            corpus_navy.append(text)\n",
    "            subreddits.append(2)\n",
    "        else:\n",
    "            corpus_mario.append(text)\n",
    "            subreddits.append(3)\n",
    "        \n",
    "        # append the text to the list of all comments\n",
    "        comments.append(text)\n",
    "    \n",
    "    # get a list of words common in each corpus that are relevant to the corpus' topic\n",
    "    relevant_common_words_nerf = extract_from_corpus(corpus_nerf, ['nerf','play'])\n",
    "    relevant_common_words_uke = extract_from_corpus(corpus_nerf, ['ukulele','music'])\n",
    "    relevant_common_words_navy = extract_from_corpus(corpus_nerf, ['navy','job'])\n",
    "    relevant_common_words_mario = extract_from_corpus(corpus_nerf, ['mario','mario'])\n",
    "    \n",
    "    # combine the lists\n",
    "    all_words = relevant_common_words_nerf + relevant_common_words_uke + relevant_common_words_navy + relevant_common_words_mario\n",
    "    \n",
    "    # remove the words that are common in more than one corpus\n",
    "    global all_uni_words\n",
    "    all_uni_words = [w for w in all_words if all_words.count(w) == 1]\n",
    "    \n",
    "    # iterate through the processed comments, and generate a list where each element corresponds to one of the common words across all corpi\n",
    "    # add 1 if the word is present in the text, 0 if it is not\n",
    "    for c in comments:\n",
    "        vec_comment = np.zeros(300)\n",
    "        common_words = []\n",
    "        \n",
    "        tokens = c.split()\n",
    "        \n",
    "        for w in tokens:\n",
    "            vec_comment += getVector(w)\n",
    "        \n",
    "        for w in all_uni_words:\n",
    "            if w in tokens:\n",
    "                common_words.append(1)\n",
    "            else:\n",
    "                common_words.append(0)\n",
    "        \n",
    "        # convert this list of common words presence indicator to an array\n",
    "        common_words = np.array(common_words)\n",
    "        # append this array to the original word2vec array of that comment\n",
    "        vec_comment = np.append(vec_comment,common_words)\n",
    "\n",
    "        # add this final comment array to the list of all comment arrays\n",
    "        comment_vecs.append(vec_comment)\n",
    "    \n",
    "   # convert list of arrays of comments to an array of arrays of comments for training\n",
    "    X = np.stack(comment_vecs)\n",
    "    # convert list of subreddits to an array of subreddits\n",
    "    y = np.array(subreddits)\n",
    "\n",
    "    # define the model\n",
    "    global logreg\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    # fit the model onto the training data\n",
    "    logreg.fit(X, y)\n",
    "\n",
    "\n",
    "def classifySubreddit_test(comment):\n",
    "    \n",
    "    global stops\n",
    "    global zero_arr\n",
    "\n",
    "    # vector to store the sum of vectors of all words in the comment\n",
    "    vec_comment = np.zeros(300)\n",
    "\n",
    "    # process the comment: make it lowercase, remove punctuation and stopwords, remove words not represented in word2vec model\n",
    "    comment = comment.lower()\n",
    "\n",
    "    comment = re.sub(r'[^a-zA-Z0-9 ]', ' ', comment)\n",
    "    comment = re.sub(r' +',' ', comment)\n",
    "        \n",
    "    tokens = comment.split()\n",
    "    tokens = [w for w in tokens if (w not in stops) and not (np.array_equal(w,zero_arr))]\n",
    "\n",
    "    # sum up the vectors of all words in the processed text\n",
    "    for w in tokens:\n",
    "        vec_comment += getVector(w)\n",
    "    \n",
    "    common_words = []\n",
    "    \n",
    "    global all_uni_words\n",
    "\n",
    "    # find which of the common words from the subreddits are present, append 1 for present, 0 for not present\n",
    "    for w in all_uni_words:\n",
    "        if w in tokens:\n",
    "            common_words.append(1)\n",
    "        else:\n",
    "            common_words.append(0)\n",
    "\n",
    "    # append array representing common words to the original word2vec array of the sentence\n",
    "    vec_comment = np.append(vec_comment,np.array(common_words))\n",
    "    \n",
    "    global logreg\n",
    "    # predict subreddit using the model\n",
    "    result = logreg.predict([vec_comment])\n",
    "\n",
    "    # return the name of the subreddit\n",
    "    if result[0] == 0:\n",
    "        return \"Nerf\"\n",
    "    elif result[0] == 1:\n",
    "        return \"ukulele\"\n",
    "    elif result[0] == 2:\n",
    "        return \"newToTheNavy\"\n",
    "    else:\n",
    "        return \"MarioMaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Retrieving word2vec vectors. This may take a few minutes...\")\n",
    "model = word2vec.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables to be used in subreddit classification\n",
    "stops = stop_words.ENGLISH_STOP_WORDS\n",
    "zero_arr = np.zeros(300)\n",
    "logreg = 0\n",
    "all_uni_words = []\n",
    "\n",
    "# training file and testing file paths\n",
    "trainFile = \"redditComments_train.jsonlist\"\n",
    "testFile = \"redditComments_test.jsonlist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call training function\n",
    "classifySubreddit_train(trainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the data from the testing file\n",
    "test_data = []\n",
    "    \n",
    "# collect data from json file\n",
    "with open(testFile, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "f.close()\n",
    "\n",
    "# variable to store how many test cases we got right\n",
    "correct = 0\n",
    "\n",
    "# for loop to test the model\n",
    "for d in test_data:\n",
    "    if d['subreddit'] == classifySubreddit_test(d['body']):\n",
    "        correct+=1\n",
    "\n",
    "# calculate and print accuracy\n",
    "accuracy = correct/len(test_data)\n",
    "\n",
    "print(\"Accuracy on test set: \", accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
